---
title: 'sf vs sp spatial packages'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/ohibc/src/templates/ohibc_hdr1.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'Figs/',
                      echo = TRUE, message = FALSE, warning = FALSE)

library(rgdal)
library(sf)
library(tmap)

source('~/github/ohibc/src/R/common.R')  ### an OHIBC specific version of common.R;
  ### includes library(tidyverse); library(stringr)

```

# References

* [Simple features for R, part 1](https://cran.r-project.org/web/packages/sf/vignettes/sf1.html)
* [Simple features for R, part 2: Reading, Writing and Converting Simple Features](https://cran.r-project.org/web/packages/sf/vignettes/sf2.html)
* [Simple features for R, part 3: Manipulating Simple Feature Geometries](https://cran.r-project.org/web/packages/sf/vignettes/sf3.html)
* [Simple Features for R v0.5-1](https://www.rdocumentation.org/packages/sf/versions/0.5-1)
* [First impressions from `sf` â€“ the simple features R package](https://geographicdatascience.com/2017/01/06/first-impressions-from-sf-the-simple-features-r-package/)
* [Tidying feature geometries with `sf`](http://r-spatial.org/r/2017/03/19/invalid.html)
* [Tidy spatial data in R: using dplyr, tidyr, and ggplot2 with `sf`](http://strimas.com/r/tidy-sf/)
* [sf - plot, graticule, transform, units, cast, is](http://r-spatial.org/r/2017/01/12/newssf.html)
* unrelated but this looks rad: [mapedit - interactively edit spatial data in R](http://r-spatial.org/r/2017/01/30/mapedit_intro.html)

# Methods

## Read polygon features

Reading standard shapefiles (OHIBC regions as example) using `sf` package and `rgdal` package (which is based on `sp`).

`sf::read_sf` (or `st_read()`) uses similar grammar to `rgdal::readOGR()`, i.e. specify `dsn` and `layer` arguments rather than just the full path at once.

``` {r readOGR vs read_sf}

dir_ohibc   <- path.expand('~/github/ohibc/prep/_spatial')
ohibc_layer <- 'ohibc_rgn'

# system.time(
#   {
#     for (i in 1:50) {
#       ohibc_rgn <- rgdal::readOGR(dir_ohibc, ohibc_layer)
#     }
#   }
# )
### loading ohibc_rgn.shp 50 times = 87.489 seconds
### ~ 1.80 s average

# system.time(
#   {
#     for (i in 1:50) {
#       ohibc_rgn <- sf::read_sf(dir_ohibc, ohibc_layer)
#     }
#   }
# )
### loading ohibc_rgn.shp 50 times = 3.156 seconds
### ~ 0.063 s average

```

Loading the ohibc_rgn.shp file 50 times with each package, on Mazu:

| function call | elapsed time, 50 loads | mean load time |
|:---------     |:----------------------:|:--------------:|
| `rgdal::readOGR()` |      87.489 s     |      1.75 s    |
| `sf::read_sf()`    |       3.156 s     |     0.063 s    |

Based on this, `sf::read_sf()` is about 30 x faster than `rgdal::readOGR()`.

## Write polygon features

`sf::write_sf()` seems to have an issue with `dsn` directories with existing shapefiles.  If the only shapefile is the same one I am writing to, it seems to delete it and overwrite as default.  But if another shapefile (with another name) is in there, it chokes and gives the error:

```
Error in CPL_write_ogr(obj, dsn, layer, driver, as.character(dataset_options),  : 
  Dataset already exists.
```

You can get around this by creating a temp folder, writing to that, then copying over when you are done, but hopefully a future version of the `sf` package will just fix this to a better system.

``` {r writeOGR vs write_sf}

# dir_ohibc   <- path.expand('~/github/ohibc/prep/_spatial')
# ohibc_layer <- 'ohibc_rgn'
# 
# dir.create('sf_sp_testing')
# dir_tmpsave <- path.expand('~/github/ohibc/sf_sp_testing')
# 
# ohibc_spdf <- rgdal::readOGR(dir_ohibc, ohibc_layer)

# system.time(
#   {
#     for (i in 1:50) {
#       writeOGR(ohibc_spdf, 
#                dir_tmpsave, ohibc_layer, 
#                driver = 'ESRI Shapefile',
#                overwrite_layer = TRUE)
#     }
#   }
# )
### writing ohibc_rgn.shp 50 times = 125.551 seconds
### ~ 2.51 s average

# ohibc_sf   <- sf::read_sf(dir_ohibc, ohibc_layer)

# system.time(
#   {
#     for (i in 1:50) {
#       write_sf(ohibc_sf,
#                dir_tmpsave, ohibc_layer,
#                driver = 'ESRI Shapefile')
#     }
#   }
# )
### writing ohibc_rgn.shp 50 times = 58.179 seconds
### ~ 1.16s average

```

Writing the `ohibc_rgn.shp` file 50 times with each package, on Mazu:

| function call | elapsed time, 50 writes | mean write time |
|:---------     |:-----------------------:|:---------------:|
| `rgdal::writeOGR()` |      125.551 s    |       2.51 s    |
| `sf::write_sf()`    |       58.179 s    |       1.16 s    |

Based on this, `sf::write_sf()` is about 2.2 x faster than `rgdal::writeOGR()`.

## Get data from spatial feature

Let's take a spatial data object and extract the attribute table as a data frame.  In SpatialPolygonsDataFrame objects, the data is in the `@data` slot and comes right out as a `data.frame` object.  In an `sf` object, the object itself is like a modified data.frame with a geometry column.  So for this comparison, I'll convert the `sf` object to a data.frame and strip the geometry.

Because of those extra operations, it's much slower (~ 2000 x slower).  But this probably isn't a big issue.  On the other hand, `setdiff(x, y)` shows the two resulting dataframes are identical.

``` {r get attributes}

# dir_pfmsa   <- file.path(dir_M, 'git-annex/bcprep/_raw_data/dfo_khunter',
#                          'management_boundaries/d2016/pac_fishery_mgmt_subareas')
# pfmsa_layer <- 'DFO_BC_PFMA_SUBAREAS_50K_V3_1'
# 
# pfmsa_spdf <- rgdal::readOGR(dir_pfmsa, pfmsa_layer)

# system.time(
#   {
#     for (i in 1:1e8) {
#       x <- pfmsa_spdf@data
#     }
#   }
# )
### getting data from pfmsa_spdf 100 million times = 20.264 seconds
### = 2.0264e-07 s each

# pfmsa_sf   <- sf::read_sf(dir_pfmsa, pfmsa_layer)

# system.time(
#   {
#     for (i in 1:1e5) {
#       y <- pfmsa_sf %>%
#         as.data.frame() %>%
#         select(-geometry)
#     }
#   }
# )
### getting data from pfmsa_sf 100,000 times = 46.705 seconds
### = 4.6705e-4 s each

```

## Functions operating on features

### Area calculation

`sf::st_area()` vs `rgeos::gArea()`

* `rgeos::gArea()`:
    * needs `byid = TRUE` if you want areas for each feature
    * returns vector of unitless numbers
* `sf::st_area()`:
    * returns areas for each feature by default
    * returns vector of class 'units' with numbers.

``` {r load features for examples}
dir_ohibc   <- path.expand('~/github/ohibc/prep/_spatial')
ohibc_layer <- 'ohibc_rgn'

ohibc_spdf <- rgdal::readOGR(dir_ohibc, ohibc_layer)
ohibc_sf   <- sf::read_sf(dir_ohibc, ohibc_layer)


dir_pfmsa   <- file.path(dir_M, 'git-annex/bcprep/_raw_data/dfo_khunter',
                         'management_boundaries/d2016/pac_fishery_mgmt_subareas')
pfmsa_layer <- 'DFO_BC_PFMA_SUBAREAS_50K_V3_1'

pfmsa_spdf <- rgdal::readOGR(dir_pfmsa, pfmsa_layer)
pfmsa_sf   <- sf::read_sf(dir_pfmsa, pfmsa_layer)

```

``` {r get area}

x <- rgeos::gArea(ohibc_spdf, byid = TRUE)
x
y <- sf::st_area(ohibc_sf)
y

```
```
x <- rgeos::gArea(ohibc_spdf, byid = TRUE)
#           0            1            2            3            4            5            6            7 
# 28162406566   8159221123  17519936788  46053741067  20517013399  13731588963   3210947740 315888121302
 
y <- sf::st_area(ohibc_sf)
# Units: m^2
# [1]  28162406566   8159221123  17519936788  46053741067  20517013399  13731588963   3210947740 315888121302
```

To compare the results, coercing the `units` vector into numeric, and finding the difference between the two:

``` {r, echo = TRUE} 
x - as.numeric(y)
```

```
x - as.numeric(y)
           0             1             2             3             4             5             6             7 
7.247925e-05 -5.655289e-04  8.659363e-04 -1.449585e-04 -7.629395e-04  1.716614e-04  4.243851e-05  3.051758e-04 
```
Not identical, but pretty damn small differences proportionally:

``` {r, echo = TRUE} 
(x - as.numeric(y)) / x
```

### Simplify and Validity check

It would be good to test the validity check with a known invalid polygon shapefile.  Performance wise, `rgeos::gSimplify()` and `sf::st_simplify()` are similar in elapsed time, as are `rgeos::gIsValid()` and `sf::st_is_valid()`.

``` {r check validity}

# system.time({
#   for (i in 1:10) {
#     tmp_spdf <- pfmsa_spdf %>%
#       rgeos::gSimplify(tol = 5)
#   }
# }) ### 2.48 s per iteration

# system.time({
#   for (i in 1:10) {
#     x <- rgeos::gIsValid(tmp_spdf, byid = TRUE)
#   }
# }) ### 1.5 s per iteration

# system.time({
#   for (i in 1:10) {
#     tmp_sf <- pfmsa_sf %>%
#       sf::st_simplify(dTolerance = 5)
#   }
# }) ### 2.26 s per iteration

# system.time({
#   for (i in 1:10) {
#     y <- sf::st_is_valid(tmp_sf)
#   }
# }) ### 1.88 s per iteration.

```

### Intersection of two features

`rgeos::gIntersection()` returns a SpatialPolygons object but loses the dataframe.  `raster::intersect()` (which I believe is built on `rgeos::gIntersection()`) returns a SpatialPolygonsDataFrame and keeps the data from both objects.  `sf::st_intersection` returns the `sf` object of intersecting geometries with data from both objects included.

Performance-wise, `st_intersection` and `gIntersection` are pretty close to the same, while `raster::intersect` is considerably slower.  But `st_intersection`'s output includes attributes.

* `rgeos::gIntersection()`: 167 s
* `raster::intersect()`:  263 s
* `sf::st_intersection`: 182 s

``` {r intersecting}
# system.time({
#   x <- rgeos::gIntersection(ohibc_spdf, pfmsa_spdf, byid = TRUE)
# })
#    user  system elapsed 
# 166.976   0.108 167.050 

# system.time({
#   y <- raster::intersect(ohibc_spdf, pfmsa_spdf)
# })
#    user  system elapsed 
# 263.104   0.192 263.242 

# system.time({
#   z <- sf::st_intersection(ohibc_sf, pfmsa_sf)
# })
#    user  system elapsed 
# 181.836   0.212 181.996 
```

### Create a Buffer

Testing `rgeos::gBuffer()`, `raster::buffer()`, and `sf::st_buffer()`.  
 
`sf::st_buffer()` seems significantly faster than `rgeos::gBuffer()` and especially `raster::buffer()`, and has the advantage of keeping all the attributes from the objects.

* `rgeos::gBuffer()`: 165 s
* `raster::buffer()`: 534 s
* `sf::st_buffer()`: 102 s (1.6x faster than `rgeos::gBuffer()` and 5x faster than `raster::buffer()`)
 
``` {r buffering}
# system.time({
#   x <- rgeos::gBuffer(ohibc_spdf, width = 1000, byid = TRUE)
# })
#    user  system elapsed 
# 150.572  14.740 165.263 

# system.time({
#   y <- raster::buffer(ohibc_spdf, width = 1000)
# })
#    user  system elapsed 
# 453.860  80.340 534.053 

# system.time({
#   z <- sf::st_buffer(ohibc_sf, dist = 1000)
# })
 #   user  system elapsed 
 # 98.584   3.152 101.732 
```

## Plotting

### ggplot2 vs tmap: time to create basic plot

***NOTE:*** This uses the ggplot2 installed from the `sf` branch...

These are all based on a reasonably simple shapefile.

* `ggplot2` and `Spatial*` objects: __3.357 seconds__
    * Using `ggplot2` to plot `Spatial*` objects requires using `broom::tidy()` or `ggplot2::fortify()` (on its way out) on the object to turn it into a dataframe.  This loses the data and is annoying.
    * It also messes up the plots and fills in holes.
    * Don't do it.
* `tmap` and `Spatial*` objects: __53.912 seconds__
    * no need to modify the `Spatial*` object.
    * use `tm_polygons`, `tm_borders`, or `tm_fill`
    * slower than `ggplot2`, but `ggplot2` is terrible for these objects.
* `ggplot2` and `sf` objects: __1.860 seconds__
    * no need to modify the `sf` object.  `sf` objects are pretty much dataframes already!
    * use `geom_sf()`
    * even in a non-degree CRS, it plots the degrees grid behind it.  Cool!
    * remarkably faster and looks good!  new favorite?
* `tmap` and `sf` objects: __53.635 seconds__
    * no need to modify the `sf` object.
    * use `tm_polygons`, `tm_borders`, or `tm_fill`
    * slower than `ggplot2`; about the same as `tmap` for `Spatial*` objects.

``` {r plotting with ggplot2}

# system.time({
  ohibc_spdf_fort <- broom::tidy(ohibc_spdf)
  x <- ggplot(ohibc_spdf_fort, aes(x = long, y = lat)) +
    geom_polygon() +
    labs(title = 'ggplot2 and Spatial*')
  print(x)
# })
#  user  system elapsed 
# 2.384   0.976   3.357

# system.time({
  y <- tm_shape(ohibc_spdf) +
    tm_polygons() +
    tm_layout(title = 'tmap and Spatial*')
  print(y)
# })
#   user  system elapsed 
# 53.752   0.176  53.912 

# system.time({
  z <- ggplot(ohibc_sf) +
    geom_sf() +
    labs(title = 'ggplot2 and sf')
  print(z)
# })
#  user  system elapsed 
# 1.856   0.004   1.860 

# system.time({
  w <- tm_shape(ohibc_sf) +
    tm_polygons() +
    tm_layout(title = 'tmap and sf')
  print(w)
# })
#   user  system elapsed 
# 53.424   0.228  53.635 

```


## geojson vs shp vs other formats

I think there are three arenas in which we want to consider our spatial data:

* saved on disk, or reading from/writing to disk
* in working memory
* in visualizing, especially for interactive maps

### Disk size

For our purposes, with Mazu, disk size is not super important.  For the mid-size OHIBC regions shapefile, let's read it in and re-save as geojson:

``` {r file size geojson}
library(geojsonio)
geojsonio::geojson_write(ohibc_spdf, geometry = 'polygon', file = 'ohibc_rgn_tmp.geojson')

file.info(file.path(dir_ohibc, 'ohibc_rgn.shp')) ### 9612748 bytes (9.6 MB)
file.info('ohibc_rgn_tmp.geojson') ### 31108544 bytes (31 MB)

unlink('ohibc_rgn_tmp.geojson')
```

The .geojson is actually much larger! so no benefits in saving in that format.  We already saw the speed improvements in reading the file using read_sf.

### RAM

In working memory, it seems like we are talking either `sp` objects (old school) or `sf` objects (new school).  [This page on Leaflet](https://rstudio.github.io/leaflet/json.html) seems to indicate that geojson files just get read in as `sp` objects anyway, so reading in a geojson is no different from reading in a shapefile.  How much RAM does an `sf` object take up compared to an `sp` object?

``` {r check_object_size}
object.size(ohibc_spdf) ### 16028856, 16 MB
object.size(ohibc_sf) ### 10655880, 10 MB
```

So a little smaller, by about the same proportion that the buffer test seemed to show in speed improvements.

### Interactive maps

The key here is just to down-res the maps significantly.  Here's a cool site that provides an interface for shrinking map files: http://mapshaper.org/.  You can also use `gSimplify` (in the `rgeos` package, for `sp` objects) or `st_simplify` (in the `sf` package, for `sf` objects).  In ohi-global, there is a script lurking to simplify polygons as well; it partly takes advantage of a variant of `gSimplify`, but also identifies and deletes thousands of tiny polygons in the global shapefile that are way too small to worry about on a map.

### geojson conclusions:

Don't worry about it.  Stick with `.shp` files.  Make super simplified versions of maps for interactive displays.

## Rasterizing (fasterize vs gdal_rasterize)

The raster package is largely dependent on the `sp` package, and is compatible with `sp` objects.  `raster::rasterize()` takes an `sp` object in memory and converts to a `Raster*` object.

`gdalUtils::gdal_rasterize` is independent of `sp`, instead based on GDAL and essentially running a system call at the command line.  It takes a spatial file on disk (e.g. .shp), and rasterizes it to a raster on disk (e.g. .tif), and can also return it as a `Raster*` object into working memory.  

`fasterize::fasterize()` takes an `sf` object and rasterizes it into working memory, as a `Raster*` object.

``` {r rasterize with raster}
rast_base <- raster::raster(file.path(dir_ohibc, 'raster/ohibc_rgn_raster_1000m.tif'))
# 
# system.time({
  rast_from_raster <- raster::rasterize(ohibc_spdf, rast_base) #, field = 'rgn_id'])
# }) ### ~ 30 seconds and can't even get it to work with the field argument.
# 
raster::writeRaster(rast_from_raster, 'rast_from_raster.tif', overwrite = TRUE)
# file.info('rast_from_raster.tif') ### 1184533 bytes

```

``` {r rasterize with gdal_rast2}
rast_base <- raster::raster(file.path(dir_ohibc, 'raster/ohibc_rgn_raster_1000m.tif'))

source('~/github/ohibc/src/R/rast_tools.R')
source_file <- file.path(dir_ohibc, 'ohibc_rgn.shp')
dest_file <- file.path(getwd(), 'rast_from_gdal_rast2.tif')

# system.time({
  rast_from_gdal_rast2 <- gdal_rast2(src = source_file, rast_base = rast_base, dst = dest_file, value = 'rgn_id', override_p4s = TRUE)
# }) ### ~ 7 seconds
# 
# file.info('rast_from_gdal_rast2.tif') ### 1177035 bytes
### Note that the raw gdal_rasterize() file is more like 20 MB, but I re-save
### it using writeRaster to get the benefits of LZW compression

```

``` {r rasterize with fasterize}
rast_base <- raster::raster(file.path(dir_ohibc, 'raster/ohibc_rgn_raster_1000m.tif'))

# system.time({
  rast_from_fasterize <- fasterize::fasterize(ohibc_sf, rast_base, field = 'rgn_id')
# }) ### ~ 0.054 seconds

raster::writeRaster(rast_from_fasterize, 'rast_from_fasterize.tif', overwrite = TRUE)

# file.info('rast_from_fasterize.tif') ### 1184533 bytes

```

### Plotting rasters:

1. raster generated using `raster::rasterize()`; note that Haida Gwaii is mostly filled in, and the numbers do not correspond with the region IDs (e.g. HG should be 2, not 4)
2. raster generated using `gdalUtils::gdal_rasterize()`, with my own custom wrapper `gdal_rast2()` to make the inputs a little more friendly.
3. raster generated using `fasterize::fasterize()`

Subtracting 2 from 3 results in a raster of zeros: they are identical!

Note: `tmap` seems pretty fast for rasters, just not so much for polygons.

``` {r plot_rasters}

rast_trimmed <- raster::extent(c(xmin = 100000, xmax = 1200000, ymin = 220000, ymax = 1200000))
r1 <- raster::raster('rast_from_raster.tif') %>%
  raster::crop(rast_trimmed)
r2 <- raster::raster('rast_from_gdal_rast2.tif') %>%
  raster::crop(rast_trimmed)
r3 <- raster::raster('rast_from_fasterize.tif') %>%
  raster::crop(rast_trimmed)

r4 <- r3 - r2

tm_shape(r1) + tm_raster(palette = 'Set3') + tm_layout(title = 'raster::raster()')
tm_shape(r2) + tm_raster(palette = 'Set3') + tm_layout(title = 'gdalUtils::gdal_rasterize()')
tm_shape(r3) + tm_raster(palette = 'Set3') + tm_layout(title = 'fasterize::fasterize()')

unlink(c('rast_from_raster.tif',
         'rast_from_gdal_rast2.tif',
         'rast_from_fasterize.tif'))
```

### Raster conclusions:

| method | elapsed time | file size | notes                     |
| ------ | ------------ | --------- | ------------------------- |
| raster::rasterize | 30 sec | 1.2 MB | requires `sp` object; errors when specifying field; errors in resulting raster |
| gdalUtils::gdal_rasterize | 7 sec | 1.2 MB * | only works from saved spatial file; raw saved file is more like 20 MB without compression |
| fasterize::fasterize | 0.054 sec | 1.2 MB | requires `sf` object; WAYYYYYYY faster; returns regular `Raster*` object but doesn't save to disk |

So: use `fasterize::fasterize` where possible.  Do not use `raster::rasterize()` without carefully checking the results!!!

