---
title: 'OHIBC data prep: Salmon'
author: "*Compiled on `r date()` by `r Sys.info()['user']`*"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 3
    toc_float: yes
    number_sections: true
    theme: cerulean
    highlight: haddock
    includes: 
      in_header: '~/github/ohibc/src/templates/ohibc_hdr1.html'
  pdf_document:
    toc: true
---

``` {r setup, echo = TRUE, message = FALSE, warning = FALSE}

knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = 'Figs/',
                      echo = TRUE, message = FALSE, warning = FALSE)

library(raster)
library(sf)

source('~/github/ohibc/src/R/common.R')  ### an OHIBC specific version of common.R

scenario <- 'v2017'
goal     <- 'sal'
dir_git  <- path.expand('~/github/ohibc')
dir_goal <- file.path(dir_git, 'prep', goal, scenario)
dir_spatial <- file.path(dir_git, 'prep/_spatial')

dir_data_bc  <- file.path(dir_M, 'git-annex/bcprep', '_raw_data')

library(provRmd); prov_setup()

reload <- FALSE


```

# Summary: OHIBC Salmon

This script prepares layers Salmon sub-goals for food provision and First Nations Resource Access Opportunity in British Columbia's coastal regions.  

-----

# Data sources

* DFO Salmon catch and escapement data

-----

# Methods

## Extract catch and escapement info

``` {r}

salmon_raw_file <- file.path(dir_goal, 'int/salmon_data_raw.csv')


salmon_sum_xls  <- file.path(dir_data_bc, 'dfo_salmon/Salmon Data Summary.xlsx')
salmon_sum_cols <- readxl::read_excel(salmon_sum_xls, 1) %>%
  clean_df_names()
salmon_sum_avail <- readxl::read_excel(salmon_sum_xls, 2) %>%
  clean_df_names()

salmon_xls <- file.path(dir_data_bc, 'dfo_salmon/OHI Data Canada 29 Dec 2017.xlsx')

ltr_to_num <- function(x) {
  if(is.na(x)) return(NA)
  if(nchar(x) == 1) {
    return (which(letters == tolower(x)))
  }
  if(nchar(x) == 2) {
    x1 <- substr(x, 1, 1)
    x2 <- substr(x, 2, 2)
    x1_num <- which(letters == tolower(x1))
    x2_num <- which(letters == tolower(x2))
    return( x1_num * 26 + x2_num)
  }
}

get_col_data <- function(y, type = 'c') {
  if(is.na(y)) {
    return('no column given')
  } else if (type == 'c') {
    return(as.character(data_raw[ , y]))
  } else {
    return(data_raw[ , y])
  }
}

salmon_stocks <- salmon_sum_cols$stock_name
  ### NOTE: Stikine sheet has two stocks; go by stock rather than sheet!
salmon_list <- vector('list', length = length(salmon_stocks))

for(i in seq_along(salmon_stocks)) { ### i <- 10
  
  col_info <- salmon_sum_cols %>%
    filter(stock_name == salmon_stocks[i])
  avail_info <- salmon_sum_avail %>%
    filter(stock_name == salmon_stocks[i])

  skip_rows     <- col_info$data_start - 1
  ### Column IDs are given as Excel column letters; convert to number
  esc_tgt_col   <- ltr_to_num(col_info$escapement_target_col_id)
  esc_col       <- ltr_to_num(col_info$escapement_col_id)
  catch_tgt_col <- ltr_to_num(col_info$catch_target_col_id)
  catch_col     <- ltr_to_num(col_info$catch_col_id)
  year_col      <- ltr_to_num(col_info$yr_col)
  
  data_raw <- readxl::read_excel(salmon_xls, 
                                 sheet = col_info$sheet_name,
                                 skip = skip_rows,
                                 col_names = FALSE) %>%
    as.matrix()
  
  stock_data <- data.frame('stock'     = col_info$stock_name,
                           'avail'     = avail_info$availability,
                           'year'      = get_col_data(year_col),
                           'esc_tgt'   = get_col_data(esc_tgt_col),
                           'escapes'   = get_col_data(esc_col),
                           'catch_tgt' = get_col_data(catch_tgt_col),
                           'catch'     = get_col_data(catch_col))

  salmon_list[[i]] <- stock_data

}

salmon_data_raw <- bind_rows(salmon_list) %>%
  filter(!is.na(year))

write_csv(salmon_data_raw, salmon_raw_file)

```

### Exclude stocks with inadequate data

From the raw salmon data, we will include only stocks with columns for (catch & catch target) and/or (escapement & escapemenet target).  Stocks with neither will be dropped.  
``` {r identify stocks with inadequate data}

salmon_data_raw <- read_csv(salmon_raw_file)

stocks_dropped <- salmon_data_raw %>%
  filter(avail == 3) %>%
  select(stock) %>% distinct()

knitr::kable(stocks_dropped, caption = 'Stocks dropped due to inadequate data')

```

### Clean data for catch, escapes, and targets

Due to the methods for extracting data from the spreadsheets, some columns have text-based data that needs to be extracted or cleaned.  Extract embedded data from text where applicable, and use "as.numeric" to convert text w/o data to NA.

``` {r clean up the data, eval = FALSE}

salmon_data_raw <- read_csv(salmon_raw_file)

stocks <- salmon_data_raw %>%
  filter(avail != 3)

cat('\nProblems in "year":\n')
print(stocks$year[str_detect(stocks$year, '[^0-9]')])
cat('\nProblems in "esc_tgt":\n')
print(stocks$esc_tgt[str_detect(stocks$esc_tgt, '[^0-9\\.]')] %>% unique())
cat('\nProblems in "escapes":\n')
print(stocks$escapes[str_detect(stocks$escapes, '[^0-9\\.]')] %>% unique())
cat('\nProblems in "catch_tgt":\n')
print(stocks$catch_tgt[str_detect(stocks$catch_tgt, '[^0-9\\.]')] %>% unique())
cat('\nProblems in "catch":\n')
print(stocks$catch[str_detect(stocks$catch, '[^0-9\\.]')] %>% unique())

```

``` {r}

salmon_data_raw <- read_csv(salmon_raw_file)

stocks <- salmon_data_raw %>%
  filter(avail != 3) %>%
  mutate(year = as.integer(year),       
           ### no actual problems here
         escapes = as.numeric(escapes), 
           ### no actual problems here
         esc_tgt = str_replace_all(esc_tgt, '[^0-9-]', ''), 
           ### ditch non-digits, spaces, and commas
         esc_tgt = str_replace_all(esc_tgt, '-.*', ''),     
           ### use hyphen to drop anything after
         esc_tgt = as.numeric(esc_tgt), 
           ### no problems remaining
         catch_tgt = ifelse(str_detect(catch_tgt, '[^0-9\\.]'), 
                                           str_extract(catch_tgt, '[0-9]*\\%'), 
           ### if non-digits in string, extract number attached to %; this extracts the 5 from "3-5%"
                                           catch_tgt),
           ### otherwise assign all-digits catch target field
         catch_tgt = as.numeric(str_replace(catch_tgt, '%', '')),
         catch = ifelse(catch == 'uncertain (10.1 to 19.4 depending on methods)', 10.1, catch), 
           ### manually assign this one
         catch = as.numeric(catch)) %>%
  select(-avail)

write_csv(stocks, file.path(dir_goal, 'int/salmon_data_cleaned.csv'))

```

## Write output layers

Output layers for functions.R will be simply the year, stock, E/Et, C/Ct; these will each be assigned equally across all regions.

``` {r}

stocks <- read_csv(file.path(dir_goal, 'int/salmon_data_cleaned.csv'))

yrs <- stocks$year %>% unique() %>% sort()
rgn_yrs <- data.frame(rgn_id = rep(c(1:6, 8), times = length(yrs)),
                      year   = rep(yrs, each = 7))

stock_layers <- stocks %>%
  mutate(E_Et        = (escapes / esc_tgt) %>% round(5),
         C_Ct        = (catch_tgt / catch) %>% round(5)) %>%
  filter(!(is.na(E_Et) & is.na(C_Ct))) %>% ### if both esc_score and catch_score are NA, drop the row
  select(stock, year, E_Et, C_Ct) %>%
  left_join(rgn_yrs, by = 'year')

catch_layer <- stock_layers %>%
  select(rgn_id, year, stock, C_Ct)

escapes_layer <- stock_layers %>%
  select(rgn_id, year, stock, E_Et)

write_csv(catch_layer,   file.path(dir_goal, 'output', 'sal_catch.csv'))
write_csv(escapes_layer, file.path(dir_goal, 'output', 'sal_escapes.csv'))

```

## Calculate analogs to B' and F'

___NOTE: From here, these calculations are not canon - the official calculations are stored within `functions.R`.  However, these calculations were used to determine and test those official calculations.___

For FIS wild capture fisheries, we use a combination of rescaled B/Bmsy and F/Fmsy based on thresholds and targets.

For SAL we will use catch and escapement data to generate a similar rescaled score.  To determine reference points, we examine the distribution of $E/E_{target}$ and $C/C_{target}$.

For each axis, we determine the mean and standard deviation.  By determining mean and standard deviation of the $E/E_{target}$ and $C/C_{target}$ we essentially normalize by the expected run size and so can compare runs directly rather than a more complex weighting scheme.

By calculating mean and SD across the entire dataset without grouping by species or number of assessed years, we are implicitly weighting by number of stocks and number of years per stock (e.g. holding years constant, then sockeye (eight fisheries) carry the same weight as all others combined (2 Chinook, 3 Chum, 2 Pink, 1 Coho)).

Calculating a coefficient of variation (i.e. $\sigma/\mu$) we recenter and rescale the distribution around the ideal value of 1.0.

### Rescaling $E/E_{target}$

``` {r determine_E_refs}

stocks <- read_csv(file.path(dir_goal, 'int/salmon_data_cleaned.csv'))

stocks_rescaled <- stocks %>%
  mutate(E_Et = escapes / esc_tgt)

E_dist <- stocks_rescaled %>%
  filter(!is.na(E_Et)) %>%
  summarize(mean_E_Et    = mean(E_Et),
            sd_E_Et      = sd(E_Et),
            c_v_E_Et = sd_E_Et / mean_E_Et)

knitr::kable(E_dist)
```

For escapes, we wish to penalize underescapement rather harshly, while being more lenient on overescapement.  Under or over escapement can result from poor management or targets based on poor pre-season estimates, either one of which is problematic in maintaining a sustainable fishery.

Using an approximate coefficient of variation of $c_{v,E} = 0.60$ and a max overescapement penalty of $\delta_E = .25$, we set thresholds away from the target (i.e. 1.0) at 1 std dev below, 1 std dev above, and 2 std devs above.

\begin{align*}
  E' &= \begin{cases}
    0.0                             &\text{ when } \frac{E}{E_t} < (1.0 - c_{v,E})\\
    E_{0,1}' + m_{E1} \frac{E}{E_t} &\text{ when } (1.0 - c_{v,E}) \leq \frac{E}{E_t} < 1.0\\
    1.0                             &\text{ when } 1.0 \leq \frac{E}{E_t} < (1.0 + c_{v,E})\\
    E_{0,2}' + m_{E2} \frac{E}{E_t} &\text{ when } (1.0 + c_{v,E}) \leq \frac{E}{E_t} < (1.0 + 2 c_{v,E})\\
    \delta_E                        &\text{ when } \frac{E}{E_t} \geq (1.0 + 2 c_{v,E})
  \end{cases}\\
  \text{Where: }\\
    m_{E1}      &= \frac{1}{c_{v,E}}         = \frac{1}{.6}        =  1.67 \text{ for } c_{v,E} = 0.60\\
    E_{0,1}' &= 1 - m_{E1}                   = \frac{-.4}{.6}      =  -.67 \text{ for } c_{v,E} = 0.60\\
    m_{E2}      &= -\frac{1 - \delta_E}{c_{v,E}} = -\frac{.75}{.6} = -1.25 \text{ for } c_{v,E} = 0.60\\
    E_{0,2}' &= 1 - m_{E2} (1 + c_{v,E})     = 1 - (-1.25)(1.6)    =  3.00 \text{ for } c_{v,E} = 0.60\\
\end{align*}
          
### Rescaling $C/C_{target}$

``` {r determine_C_refs}

stocks <- read_csv(file.path(dir_goal, 'int/salmon_data_cleaned.csv'))

stocks_rescaled <- stocks %>%
  mutate(C_Ct = catch / catch_tgt)
    ### How to deal with years with zero catch targets? drop these? use mean 
    ### of other years? set at some high value?

C_dist <- stocks_rescaled %>%
  filter(!is.na(C_Ct)) %>%
  filter(!is.infinite(C_Ct)) %>%
  summarize(mean_C_Ct    = mean(C_Ct),
            sd_C_Ct      = sd(C_Ct),
            sd_norm_C_Ct = sd_C_Ct / mean_C_Ct)

knitr::kable(C_dist)

```

For catch, we wish to penalize excessive catch rather harshly, while being more lenient on under-harvest.  Under or over catch, as with escapement, can result from poor management or targets based on poor pre-season estimates, either one of which is problematic in maintaining a sustainable fishery.

We calculated a coefficient of variation for catch $c_{v,C}$ at about 1.20, which is rather high.  To better match our wild-capture fisheries model, we instead set an upper buffer at $C/C_t$ = 2.0 (rather than 1 $c_{v,C}$ above).  Below $C/C_t = 1.0$ we set a buffer $\gamma_C = .40$, representing an allowance for uncertainty as well as intentional underharvest for ecological purposes (e.g. "one third for the birds" - and orcas, and bears, etc).  Below this threshold, $C'$ decreases to a minimum value of $\delta_C = 0.25$ at a catch (and thus $C/C_{target}$) of zero.

\begin{align*}
  C' &= \begin{cases}
    C_{0,1}' + m_{C1} \frac{C}{C_t}  &\text{ when } \frac{C}{C_t} < 1.0 - \gamma_C\\
    1.0                              &\text{ when } 1.0 - \gamma_C \leq \frac{C}{C_t} < 1.0\\
    2.0 - \frac{C}{C_t}              &\text{ when } 1.0 \leq \frac{C}{C_t} < 2.0\\
    0.0                              &\text{ when } \frac{C}{C_t} \geq 2.0
  \end{cases}\\
  \text{Where: }\\
    m_{C1}   &= \frac{1 - \delta_C}{1.0 - \gamma_C}  = \frac{.75}{.6} =  1.25\\
    C_{0,1}' &= \delta_C                                              =  0.25
\end{align*}

``` {r implement C_prime and E_prime functions}

calc_e_prime <- function(E_Et) {
  delta_e <- .25 ### max overescape penalty
  sigma_e <- 0.6 ### std deviation of C/Ctarget
  
  m_e1 <- 1 / sigma_e
  m_e2 <- - (1 - delta_e)/(sigma_e)
  e_0_1 <- 1 - m_e1
  e_0_2 <- 1 - m_e2 * (1 + sigma_e)
  
  e_prime <- case_when(E_Et < 1.0 - sigma_e      ~ 0,
                       E_Et < 1.0                ~ e_0_1 + m_e1 * E_Et,
                       E_Et < 1.0 + sigma_e      ~ 1,
                       E_Et < 1.0 + 2 * sigma_e  ~ e_0_2 + m_e2 * E_Et,
                       E_Et >= 1.0 + 2 * sigma_e ~ delta_e,
                       TRUE                      ~ NA_real_)
}

  calc_c_prime <- function(C_Ct) {
    delta_c <- .25 ### max undercatch penalty
    gamma_c <- 0.4 ### undercatch buffer

    m_c1 <- (1 - delta_c) / (1 - gamma_c)
    c_0_1 <- delta_c

    c_prime <- case_when(C_Ct < 1.0 - gamma_c  ~ c_0_1 + m_c1 * C_Ct,
                         C_Ct < 1.0            ~ 1.0,
                         C_Ct < 2.0            ~ 2.0 - C_Ct,
                         C_Ct >= 2.0           ~ 0,
                         TRUE                  ~ NA_real_)
  }

```

Plotting the scores in the style of the FIS Kobe plots, note the effects of the asymmetric buffers around 1.0.  Penalties for exceeding catch targets and not achieving escapement targets both begin right at the targets of $\frac{C}{C_{target}} = 1.0$ and $\frac{E}{E_{target}} = 1.0$ respectively, rather than the buffer we include for uncertainty in stock assessments for FIS scores.

``` {r}

### run scores for each axis from 0 to 2.5, step 0.01 (251 instances)
plot_df <- data.frame(E_Et = rep(seq(0, 2.5, .01), times = 251),
                      C_Ct = rep(seq(0, 2.5, .01), each = 251)) %>%
  mutate(E_prime   = calc_e_prime(E_Et),
         C_prime   = calc_c_prime(C_Ct),
         SAL_score = E_prime * C_prime) %>%
  gather(facet, score, E_prime:SAL_score)

ggplot(plot_df, aes(x = E_Et, y = C_Ct)) +
  ggtheme_plot() +
  geom_raster(aes(fill = score)) +
  scale_fill_distiller(palette = 'RdYlGn', direction = 1) +
  geom_hline(yintercept = 1, color = 'grey70') +
  geom_vline(xintercept = 1, color = 'grey70') +
  facet_wrap( ~ facet) +
  labs(x = 'E/E_target', y = 'C/C_target')

```
## Calculate $E'$ and $C'$ for all stocks and years


``` {r calc_salmon_components}

stocks <- read_csv(file.path(dir_goal, 'int/salmon_data_cleaned.csv'))

stocks_scored <- stocks %>%
  rowwise() %>%
  mutate(E_Et        = (escapes / esc_tgt) %>% round(5),
         esc_score   = calc_e_prime(E_Et)  %>% round(5),
         C_Ct        = (catch_tgt / catch) %>% round(5),
         catch_score = calc_c_prime(C_Ct)  %>% round(5),
         stock_score = prod(c(esc_score, catch_score), na.rm = TRUE) %>% round(5)) %>%
  filter(!(is.na(esc_score) & is.na(catch_score))) %>% ### if both esc_score and catch_score are NA, drop the row
  ungroup() %>%
  mutate(stock_spp = case_when(str_detect(tolower(stock), 'chum') ~ 'chum',
                               str_detect(tolower(stock), 'pink') ~ 'pink',
                               str_detect(tolower(stock), 'coho') ~ 'coho',
                               str_detect(tolower(stock), 'ch')   ~ 'chinook',
                               TRUE                               ~ 'sockeye')) %>%
  select(stock, year, 
         E_Et, esc_score,
         C_Ct, catch_score, 
         stock_score)

stocks_scored %>%
  DT::datatable()

write_csv(stocks_scored, file.path(dir_goal, 'int', 'salmon_stocks_scored.csv'))

```

``` {r plot_salmon_stock_scores}

stocks_scored <- read_csv(file.path(dir_goal, 'int', 'salmon_stocks_scored.csv'))

for(stock_name in stocks_scored$stock %>% unique()) { ### stock_name <- stocks_scored$stock[1]
  stock_df <- stocks_scored %>%
    filter(stock == stock_name)
  
  score_plot <- ggplot(stock_df, aes(x = year)) +
    ggtheme_plot() +
    geom_line(aes(y = esc_score), size = 2, alpha = .6, color = 'blue') +
    geom_line(aes(y = E_Et),   size = .5, alpha = .6, color = 'blue', linetype = 'dashed') +
    geom_line(aes(y = catch_score), size = 2, alpha = .6, color = 'red4') +
    geom_line(aes(y = C_Ct),   size = .5, alpha = .6, color = 'red4', linetype = 'dashed') +
    ylim(c(0, NA)) +
    labs(title = stock_name,
         y = 'solid = rescaled score, dashed = X/X_target')
    
  stock_plot <- ggplot(stock_df, aes(x = year, y = stock_score)) +
    ggtheme_plot() +
    geom_line(size = 2, color = 'grey60', alpha = 1) +
    geom_point(size = 3, color = 'grey60', alpha = 1) +
    geom_line(size = 1, alpha = .8, color = 'red4',  aes(y = catch_score)) +
    geom_line(size = 1, alpha = .8, color = 'blue', aes(y = esc_score)) +
    ylim(c(0, 1)) +
    labs(title = '(R=catch, B=escapes, Grey=tot)',
         y = 'Score and components')

  multi_plot <- cowplot::plot_grid(score_plot, stock_plot, align = 'v')
  print(multi_plot)
}

```

## Combine scores across all BC

Because commercial salmon harvests are often brought to harbors away from the place of catch, and because we are scoring based on a subset of all commercial fisheries, we simply aggregate all salmon fishery scores to the overall BC coastal scale.

``` {r combine_scores}

stocks_scored <- read_csv(file.path(dir_goal, 'int', 'salmon_stocks_scored.csv')) %>%
  filter(!is.na(year) & year <= 2016)

combined_score <- stocks_scored %>%
  group_by(year) %>%
  summarize(sal_score = mean(stock_score, na.rm = TRUE))
  
ggplot(combined_score, aes(x = year, y = sal_score)) +
  ggtheme_plot() +
  geom_line(color = 'salmon', size = 2) +
  ylim(c(0, 1)) +
  labs(title = 'Overall SAL score',
       y = 'SAL score')

DT::datatable(combined_score)
```


-----

``` {r provenance, results = 'asis'}

prov_wrapup()

```
